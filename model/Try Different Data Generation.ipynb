{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../'\n",
    "limit = 200\n",
    "cover_imgs = sorted(glob.glob(input_dir+'Cover/*.jpg'))\n",
    "cover_imgs = cover_imgs[:limit]\n",
    "\n",
    "jmipod_imgs = sorted(glob.glob(input_dir+'JMiPOD/*.jpg'))\n",
    "jmipod_imgs = jmipod_imgs[:limit]#[-limit:]\n",
    "\n",
    "juniward_imgs = sorted(glob.glob(input_dir+'JUNIWARD/*.jpg'))\n",
    "juniward_imgs = juniward_imgs[:limit]#[-limit:]\n",
    "\n",
    "uerd_imgs = sorted(glob.glob(input_dir+'UERD/*.jpg'))\n",
    "uerd_imgs = uerd_imgs[:limit]#[-limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "print(cover_imgs[index])\n",
    "print(jmipod_imgs[index])\n",
    "print(juniward_imgs[index])\n",
    "print(uerd_imgs[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_IDs = cover_imgs + jmipod_imgs + juniward_imgs + uerd_imgs\n",
    "# random.Random(4).shuffle(list_IDs)\n",
    "\n",
    "till = int(0.8*len(cover_imgs))\n",
    "print(till)\n",
    "\n",
    "# train_IDs = []\n",
    "# test_IDs = []\n",
    "\n",
    "train_IDs = cover_imgs[:till]+juniward_imgs[:till]+uerd_imgs[:till]+jmipod_imgs[:till]\n",
    "# train_IDs = cover_imgs[:100] + juniward_imgs[100:100*2]# + uerd_imgs[:till]\n",
    "# train_IDs = [i.split('/')[-1] for i in train_IDs]\n",
    "test_IDs = cover_imgs[till:]+juniward_imgs[till:]+uerd_imgs[till:]+jmipod_imgs[till:]\n",
    "# test_IDs = cover_imgs[:50] + juniward_imgs[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_DCT(file):\n",
    "#     c_struct=jio.read( file )\n",
    "#     coverDCT = np.zeros([512,512,3]) ; coverDCT[:,:,0] = c_struct.coef_arrays[0] ; coverDCT[:,:,1] = c_struct.coef_arrays[1] ; coverDCT[:,:,2] = c_struct.coef_arrays[2]\n",
    "#     return coverDCT\n",
    "\n",
    "# # def get_image_data(image_id, image_size):\n",
    "# #     img = cv2.imread(image_id)\n",
    "# # #     h, w = image_size\n",
    "# # #     img= img[100:100+h, 100:100+w]\n",
    "# #     return cv2.resize(img, image_size)\n",
    "\n",
    "# def get_image_data(image_id, image_size, COV_DIR='/kaggle/input/alaska2-image-steganalysis/Cover/', STEG_DIR='/kaggle/input/alaska2-image-steganalysis/JUNIWARD/'):\n",
    "#     img_cov = cv2.imread(COV_DIR+image_id)\n",
    "#     img_stego = cv2.imread(STEG_DIR+image_id)\n",
    "\n",
    "# #     h, w = image_size\n",
    "# #     img= img[100:100+h, 100:100+w]\n",
    "#     return cv2.resize(img_cov, image_size), cv2.resize(img_stego, image_size)\n",
    "\n",
    "\n",
    "# def get_label(image_id):\n",
    "#     # 0 for cover, 1 for steg\n",
    "#     return 0, 1\n",
    "# #     if '/kaggle/input/alaska2-image-steganalysis/Cover/' in image_id:\n",
    "# #         return 0\n",
    "# #     else:\n",
    "# #         return 1\n",
    "\n",
    "    \n",
    "# class DataGenerator(keras.utils.Sequence):\n",
    "#     'Generates data for Keras'\n",
    "#     def __init__(self, list_IDs, batch_size=32, dim=(32,32,32), n_channels=1, \n",
    "#                  shuffle=True, number_of_labels=2):\n",
    "#         'Initialization'\n",
    "#         self.dim = dim\n",
    "#         self.batch_size = batch_size\n",
    "#         self.list_IDs = list_IDs\n",
    "#         self.n_channels = n_channels\n",
    "#         self.shuffle = shuffle\n",
    "#         self.on_epoch_end()\n",
    "#         self.number_of_labels = number_of_labels\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         'Denotes the number of batches per epoch'\n",
    "#         return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         'Generate one batch of data'\n",
    "#         # Generate indexes of the batch\n",
    "#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "#         # Find list of IDs\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "#         # Generate data\n",
    "#         X, y = self.__data_generation(list_IDs_temp)\n",
    "#         # cv2 recevrses height and width\n",
    "#         return X, y\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         'Updates indexes after each epoch'\n",
    "#         self.indexes = np.arange(len(self.list_IDs))\n",
    "#         if self.shuffle == True:\n",
    "#             np.random.shuffle(self.indexes)\n",
    "\n",
    "#     def __data_generation(self, list_IDs_temp):\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "# #         X = np.empty((self.batch_size, *self.dim[::-1], self.n_channels))\n",
    "# #         y = np.empty((self.batch_size, *self.dim[::-1]))\n",
    "#         X = np.empty((self.batch_size*2, *self.dim, self.n_channels), dtype=np.float32)\n",
    "#         y = np.empty((self.batch_size*2), dtype=int)\n",
    "        \n",
    "#         # Generate data\n",
    "#         for i, ID in enumerate(list_IDs_temp):\n",
    "#             # Store sample\n",
    "#             img_cov, img_stego = get_image_data(ID, image_size) #read_DCT(ID)\n",
    "#             X[2*i,], X[2*i+1,] = img_cov, img_stego#changeColorSpace(img, flag_YCR_CB)[:, :, 0].reshape(*image_size,1)\n",
    "#             y[2*i], y[2*i+1] = get_label(ID)\n",
    "            \n",
    "        \n",
    "#         return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_DCT(file):\n",
    "    c_struct=jio.read( file )\n",
    "    coverDCT = np.zeros([512,512,3]) ; coverDCT[:,:,0] = c_struct.coef_arrays[0] ; coverDCT[:,:,1] = c_struct.coef_arrays[1] ; coverDCT[:,:,2] = c_struct.coef_arrays[2]\n",
    "    return coverDCT\n",
    "\n",
    "def get_image_data(image_id, image_size):\n",
    "    img = cv2.imread(image_id)\n",
    "#     h, w = image_size\n",
    "#     img= img[100:100+h, 100:100+w]\n",
    "    return cv2.resize(img, image_size)\n",
    "\n",
    "def get_label(image_id):\n",
    "    if '/kaggle/input/alaska2-image-steganalysis/Cover/' in image_id:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(32,32,32), n_channels=1, \n",
    "                 shuffle=True, number_of_labels=2):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.number_of_labels = number_of_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        # cv2 recevrses height and width\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim[::-1], self.n_channels))\n",
    "#         y = np.empty((self.batch_size, *self.dim[::-1]))\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float)\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            img = get_image_data(ID, image_size) #read_DCT(ID)\n",
    "            X[i,] = img/255.#changeColorSpace(img, flag_YCR_CB)[:, :, 0].reshape(*image_size,1)\n",
    "            y[i] = get_label(ID)\n",
    "            \n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256, 256)\n",
    "# Parameters\n",
    "params = {'dim': image_size,\n",
    "          'batch_size': 8,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(train_IDs, **params)\n",
    "validation_generator = DataGenerator(test_IDs, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_generator.__getitem__(1)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(X[0] - X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1])#.reshape((*image_size))/np.max(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SRNet(_inputs):\n",
    "    penalty = 2e-4\n",
    "    mom = 0.99\n",
    "    reduction_axis = [1,2]\n",
    "    #'Layer1' \n",
    "    conv=layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "           kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(_inputs) \n",
    "    actv=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv))\n",
    "    # Layer2'\n",
    "    conv=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv)\n",
    "    actv=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv))\n",
    "    #Layer3'\n",
    "    conv1=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn2=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    res= tf.add(actv, bn2)\n",
    "    # Layer4'\n",
    "    conv1=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res, )\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn2=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    res= tf.add(res, bn2)\n",
    "    # Layer5\n",
    "    conv1=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    res= tf.add(res, bn)\n",
    "    # 'Layer6\n",
    "    conv1=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    res= tf.add(res, bn)\n",
    "    # Layer7'\n",
    "    conv1=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    res= tf.add(res, bn)\n",
    "    # with tf.variable_scope('Layer8'): \n",
    "    convs = layers.Conv2D(filters=16,  kernel_size=1, strides=2)(res)\n",
    "    convs = layers.BatchNormalization(momentum=mom)(convs)\n",
    "    conv1 = layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    pool = tf.nn.avg_pool2d(bn, ksize=[3,3], strides=[2,2], padding='SAME')\n",
    "    res= tf.add(convs, pool)\n",
    "    #             with tf.variable_scope('Layer9'):  \n",
    "    convs = layers.Conv2D(filters=64, kernel_size=1, strides=2)(res)\n",
    "    convs = layers.BatchNormalization(momentum=mom)(convs)\n",
    "    conv1=layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    pool = tf.nn.avg_pool2d(bn, ksize=[3,3], strides=[2,2], padding='SAME')\n",
    "    res= tf.add(convs, pool)\n",
    "    # with tf.variable_scope('Layer10'): \n",
    "    convs = layers.Conv2D(filters=128, kernel_size=1, strides=2, padding='SAME', kernel_initializer='he_uniform',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    convs = layers.BatchNormalization(momentum=mom)(convs)\n",
    "    conv1=layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    pool = tf.nn.avg_pool2d(bn, ksize=[3,3], strides=[2,2], padding='SAME')\n",
    "    res= tf.add(convs, pool)\n",
    "    # with tf.variable_scope('Layer11'): \n",
    "    convs = layers.Conv2D(filters=256, kernel_size=1, strides=2, padding='SAME', kernel_initializer='he_uniform',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    convs = layers.BatchNormalization(momentum=mom)(convs)\n",
    "    conv1=layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    pool = tf.nn.avg_pool2d(bn, ksize=[3,3], strides=[2,2], padding='SAME')\n",
    "    res= tf.add(convs, pool)\n",
    "    # with tf.variable_scope('Layer12'): \n",
    "    conv1=layers.Conv2D(filters=512, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(res)\n",
    "    actv1=tf.nn.relu(layers.BatchNormalization(momentum=mom)(conv1))\n",
    "    conv2=layers.Conv2D(filters=512, kernel_size=3, strides=1, padding='SAME', kernel_initializer='he_uniform',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(penalty), bias_initializer=tf.constant_initializer(0.2))(actv1)\n",
    "    bn=layers.BatchNormalization(momentum=mom)(conv2)\n",
    "    avgp = tf.reduce_mean(bn, reduction_axis,  keepdims=True )\n",
    "    \n",
    "    fc1 = layers.Flatten()(avgp)\n",
    "#     do1 = layers.Dropout(0.2)(fc1)\n",
    "    d3 = layers.Dense(1, activation='sigmoid')(fc1)\n",
    "    \n",
    "    return d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = layers.Input(shape = (*image_size, 3))\n",
    "\n",
    "# input_img = tf.cast(tf.transpose(inputs, [0, 3, 1, 2]), tf.float32)\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "sr_ = Model(input_img, SRNet(input_img))\n",
    "# sr_ = tf.keras.models.load_model('/kaggle/working/SRNet_256_256_v1.h5')\n",
    "\n",
    "sr_.compile(loss=keras.losses.binary_crossentropy, \n",
    "                    metrics=METRICS, optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_ = tf.keras.models.load_model('SRNet_256_256_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr_.evaluate(training_generator, verbose=2)\n",
    "# sr_.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 25:\n",
    "        return 1e-3\n",
    "    else:\n",
    "        return 1e-4# * tf.math.exp(0.1 * (10 - epoch))\n",
    "    \n",
    "checkpoint_filepath = '/kaggle/working'\n",
    "filepath=checkpoint_filepath+\"/model-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "\n",
    "    \n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=filepath,\n",
    "    monitor='val_accuracy', verbose=1,\n",
    "    save_best_only=False, save_weights_only=False, \n",
    "    save_frequency=10)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_model_history = sr_.fit_generator(generator=training_generator, \n",
    "                                     validation_data=validation_generator, epochs=10)#, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm /kaggle/working/model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_.save('/kaggle/working/SRNet_256_256_v2.h5')\n",
    "# tf.keras.models.save_model(\n",
    "#     sr_, 'SRNet_256_256_v1.h5', overwrite=True, include_optimizer=True, save_format='h5',\n",
    "#     signatures=None, options=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr_model_history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(sr_model_history.history['accuracy'])\n",
    "plt.plot(sr_model_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sr_model_history.history['auc'])\n",
    "plt.plot(sr_model_history.history['val_auc'])\n",
    "plt.title('model auc')\n",
    "plt.ylabel('auc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sr_model_history.history['loss'])\n",
    "plt.plot(sr_model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000000000000001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 * (10 - i//25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "for i in range(1, 20):\n",
    "    if i%10 == 0:\n",
    "        lr/=10\n",
    "    print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
